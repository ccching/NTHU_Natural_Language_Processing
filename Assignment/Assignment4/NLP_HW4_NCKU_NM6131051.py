# -*- coding: utf-8 -*-
"""assignment4_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rbzj9VSiRRy9kmOPVJvcjxvaRKmHwSWX

# RAG using Langchain

## Packages loading & import
"""

!pip install langchain
!pip install langchain_community
!pip install langchain_huggingface
!pip install langchain_text_splitters
!pip install langchain_chroma
!pip install rank-bm25
!pip install huggingface_hub
!pip install faiss-gpu

import os
import json
import bs4
import nltk
import torch
import pickle
import numpy as np

# from pyserini.index import IndexWriter
# from pyserini.search import SimpleSearcher
from numpy.linalg import norm
from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize

from langchain_community.llms import Ollama
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import JinaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain.docstore.document import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import WebBaseLoader
from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer

from tqdm import tqdm

nltk.download('punkt')
nltk.download('punkt_tab')

"""## Hugging face login
- Please apply the model first: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
- If you haven't been granted access to this model, you can use other LLM model that doesn't have to apply.
- You must save the hf token otherwise you need to regenrate the token everytime.
- When using Ollama, no login is required to access and utilize the llama model.
"""

from huggingface_hub import login

hf_token = ""
login(token=hf_token, add_to_git_credential=True)

!huggingface-cli whoami

"""## TODO1: Set up the environment of Ollama

### Introduction to Ollama
- Ollama is a platform designed for running and managing large language models (LLMs) directly **on local devices**, providing a balance between performance, privacy, and control.
- There are also other tools support users to manage LLM on local devices and accelerate it like *vllm*, *Llamafile*, *GPT4ALL*...etc.

### Launch colabxterm
"""

# Commented out IPython magic to ensure Python compatibility.
# TODO1-1: You should install colab-xterm and launch it.
# Write your commands here.
!pip install colab-xterm #https://pypi.org/project/colab-xterm/
# %load_ext colabxterm
# TODO1-2: You should install Ollama.
# You may need root privileges if you use a local machine instead of Colab.
# !curl -fsSL https://ollama.com/install.sh | sh

# Commented out IPython magic to ensure Python compatibility.
# %xterm

# TODO1-3: Pull Llama3.2:1b via Ollama and start the Ollama service in the xterm
# Write your commands in the xterm
# !ollama pull llama3.2:1b

"""## Ollama testing
You can test your Ollama status with the following cells.
"""

# Setting up the model that this tutorial will use
MODEL = "llama3.2:1b" # https://ollama.com/library/llama3.2:3b
from langchain.embeddings import SentenceTransformerEmbeddings

# EMBED_MODEL = SentenceTransformerEmbeddings(model_name="all-mpnet-base-v2")
EMBED_MODEL = "jinaai/jina-embeddings-v2-base-en"

# Initialize an instance of the Ollama model
llm = Ollama(model=MODEL)
# Invoke the model to generate responses
response = llm.invoke("What is the capital of Taiwan?")
print(response)

"""## Build a simple RAG system by using LangChain

### TODO2: Load the cat-facts dataset and prepare the retrieval database
"""

!wget https://huggingface.co/ngxson/demo_simple_rag_py/resolve/main/cat-facts.txt

# TODO2-1: Load the cat-facts dataset (as `refs`, which is a list of strings for all the cat facts)
# Write your code here
with open("cat-facts.txt", "r") as f:
    refs = f.readlines()

from langchain_core.documents import Document
docs = [Document(page_content=doc, metadata={"id": i}) for i, doc in enumerate(refs)]

# Create an embedding model
model_kwargs = {'trust_remote_code': True}
encode_kwargs = {'normalize_embeddings': False}
embeddings_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# TODO2-2: Prepare the retrieval database
# You should create a Chroma vector store.
# search_type can be “similarity” (default), “mmr”, or “similarity_score_threshold”
# vector_store = Chroma.from_documents(
#     # Write your code here
#     documents=docs,
#     embedding=embeddings_model,
#     persist_directory="./chroma_db"
# )
from langchain.vectorstores import FAISS
vector_store = FAISS.from_documents(docs, embeddings_model)
retriever = vector_store.as_retriever(
    # Write your code here
    search_type="similarity",
    search_kwargs={"k": 7}
)

"""### Prompt setting"""

# TODO3: Set up the `system_prompt` and configure the prompt.
system_prompt = (
    "Thoroughly analyze the provided context to extract the precise answer to the question. "
    "Focus on identifying keywords and phrases that directly address the question. "
    "Avoid making assumptions or providing information not found in the context. "
    "Use the given context to answer the question. "
    "Please note: "
    "1. Base your answers solely on the context, not your prior knowledge. "
    "2. Keep your answers concise and to the point, avoiding unnecessary verbosity. "
    "3.Do not provide ranges unless the context explicitly includes a range."
    "Context: {context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

# 使用chatgpt來生成不同的prompt
# baseline_prompt = (
#     "Thoroughly analyze the provided context to extract the precise answer to the question. "
#     "Focus on identifying keywords and phrases that directly address the question. "
#     "Avoid making assumptions or providing information not found in the context. "
#     "Use the given context to answer the question. "
#     "Please note: "
#     "1. Base your answers solely on the context, not your prior knowledge. "
#     "2. Keep your answers concise and to the point, avoiding unnecessary verbosity. "
#     "Context: {context}"
# )
# detailed_instruction_prompt = (
#     "You are an expert cat fact analyzer. When answering questions: "
#     "- Read the entire context carefully "
#     "- Extract the most relevant information "
#     "- Prioritize direct quotes from the context "
#     "- If no exact answer is found, state 'Information not available' "
#     "- Provide a brief, clear, and precise answer "
#     "Context: {context}"
# )
# strict_matching_prompt = (
#     "Extract answers ONLY if they appear verbatim in the context. "
#     "- Do not paraphrase "
#     "- Do not infer "
#     "- If no exact match exists, respond with 'No matching information found' "
#     "Context: {context}"
# )
# creative_prompt = (
#     "Analyze the context creatively. While maintaining accuracy: "
#     "- Use contextual clues to derive answers "
#     "- If direct information is absent, provide a reasoned inference "
#     "- Explain your reasoning briefly "
#     "- Aim for comprehensive understanding "
#     "Context: {context}"
# )
# verbose_prompt = (
#     "Provide a comprehensive analysis of the question using the given context: "
#     "- Explain the context in detail "
#     "- Trace the reasoning for your answer "
#     "- Include any relevant background information "
#     "- Be thorough and informative "
#     "Context: {context}"
# )

"""- For the vectorspace, the common algorithm would be used like Faiss, Chroma...(https://python.langchain.com/docs/integrations/vectorstores/) to deal with the extreme huge database."""

# TODO4: Build and run the RAG system
# TODO4-1: Load the QA chain
# You should create a chain for passing a list of Documents to a model.
question_answer_chain = create_stuff_documents_chain(llm = llm,prompt=prompt)# Write your code here

# TODO4-2: Create retrieval chain
# You should create retrieval chain that retrieves documents and then passes them on.
chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=question_answer_chain)# Write your code here

# Question (queries) and answer pairs
# Please do not modify this cell.
queries = [
    "How much of a day do cats spend sleeping on average?",
    "What is the technical term for a cat's hairball?",
    "What do scientists believe caused cats to lose their sweet tooth?",
    "What is the top speed a cat can travel over short distances?",
    "What is the name of the organ in a cat's mouth that helps it smell?",
    "Which wildcat is considered the ancestor of all domestic cats?",
    "What is the group term for cats?",
    "How many different sounds can cats make?",
    "What is the name of the first cat in space?",
    "How many toes does a cat have on its back paws?"
]
answers = [
    "2/3",
    "Bezoar",
    "a mutation in a key taste receptor",
    ["31 mph", "49 km"],
    "Jacobson’s organ",
    "the African Wild Cat",
    "clowder",
    "100",
    ["Felicette", "Astrocat"],
    "four",
]

counts = 0

for i, query in enumerate(tqdm(queries, desc="Processing Queries")):
    # TODO4-3: Run the RAG system
    response = chain.invoke({"input": query})  # Write your code here
    print(f"Query: {query}\nResponse: {response['answer']}\n")
    # The following lines perform evaluations.
    # if the answer shows up in your response, the response is considered correct.
    if type(answers[i]) == list:
        for answer in answers[i]:
            if answer.lower() in response['answer'].lower():
                counts += 1
                break
    else:
        if answers[i].lower() in response['answer'].lower():
            counts += 1

# TODO5: Improve to let the LLM correctly answer the ten questions.
print(f"Correct numbers: {counts}")

# import gc

# gc.collect()
# torch.cuda.empty_cache()

# # retriever = vector_store.as_retriever(
# #     search_type="similarity",
# #     search_kwargs={"k": 7}
# # )

# # retriever = vector_store.as_retriever(
# #     search_type="mmr",
# #     search_kwargs={
# #         "k": 5,
# #         "fetch_k": 10
# #     }
# # )
# # retriever = vector_store.as_retriever(
# #     search_type="similarity_score_threshold",
# #     search_kwargs={
# #         "score_threshold": 0.5,
# #         "k": 5
# #     }
# # )
# def evaluate_rag_performance(retrieval_type):
#     if retrieval_type == "similarity":
#         retriever = vector_store.as_retriever(
#             search_type="similarity",
#             search_kwargs={"k": 5}
#         )
#     elif retrieval_type == "mmr":
#         retriever = vector_store.as_retriever(
#             search_type="mmr",
#             search_kwargs={"k": 5, "fetch_k": 15}
#         )
#     elif retrieval_type == "threshold":
#         retriever = vector_store.as_retriever(
#             search_type="similarity_score_threshold",
#             search_kwargs={"score_threshold": 0.5, "k": 5}
#         )

#
#     question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)
#     chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=question_answer_chain)

#
#     counts = 0
#     for i, query in enumerate(queries):
#         response = chain.invoke({"input": query})

#         if type(answers[i]) == list:
#             for answer in answers[i]:
#                 if answer.lower() in response['answer'].lower():
#                     counts += 1
#                     break
#         else:
#             if answers[i].lower() in response['answer'].lower():
#                 counts += 1

#     return counts

# retrieval_types = ["similarity", "mmr", "threshold"]
# for ret_type in retrieval_types:
#     print(f"Retrieval Type: {ret_type}")
#     correct_count = evaluate_rag_performance(ret_type)
#     print(f"Correct Answers: {correct_count}/10\n")

# def baseline_performance():
#     counts = 0
#     for i, query in enumerate(queries):
#         response = llm.invoke(query)

#         if type(answers[i]) == list:
#             for answer in answers[i]:
#                 if answer.lower() in response.lower():
#                     counts += 1
#                     break
#         else:
#             if answers[i].lower() in response.lower():
#                 counts += 1

#     return counts

# print("Baseline (No RAG) Performance:")
# baseline_count = baseline_performance()
# print(f"Correct Answers: {baseline_count}/10")

# from tqdm import tqdm

# def evaluate_rag_performance(prompt_template):
#
#     system_prompt = prompt_template

#     prompt = ChatPromptTemplate.from_messages([
#         ("system", system_prompt),
#         ("human", "{input}"),
#     ])

#     question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)
#     chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=question_answer_chain)


#     counts = 0
#     detailed_results = []

#     for i, query in enumerate(tqdm(queries, desc=f"Processing Queries for {prompt_template[:20]}...")):
#         response = chain.invoke({"input": query})

#         is_correct = False
#         if type(answers[i]) == list:
#             for answer in answers[i]:
#                 if answer.lower() in response['answer'].lower():
#                     is_correct = True
#                     break
#         else:
#             is_correct = answers[i].lower() in response['answer'].lower()

#         if is_correct:
#             counts += 1

#         detailed_results.append({
#             'query': query,
#             'response': response['answer'],
#             'correct': is_correct,
#             'expected_answer': answers[i]
#         })

#     return {
#         'correct_count': counts,
#         'accuracy_rate': counts / len(queries),
#         'detailed_results': detailed_results
#     }

# prompts = {
#     '基準': baseline_prompt,
#     '詳細指令': detailed_instruction_prompt,
#     '嚴格文字匹配': strict_matching_prompt,
#     '創意解讀': creative_prompt,
#     '冗長解釋': verbose_prompt
# }

# results = {}
# for name, prompt in tqdm(prompts.items(), desc="Evaluating Prompt Variants"):
#     results[name] = evaluate_rag_performance(prompt)

# import matplotlib.pyplot as plt
# import numpy as np

# def plot_accuracy_rates(results):
#     names = list(results.keys())
#     accuracy_rates = [results[name]['accuracy_rate'] for name in names]

#     plt.figure(figsize=(10, 6))
#     plt.bar(names, accuracy_rates)
#     plt.title('RAG Performance by Prompt Variant')
#     plt.xlabel('Prompt Variants')
#     plt.ylabel('Accuracy Rate')
#     plt.ylim(0, 1)
#     plt.xticks(rotation=45)
#     for i, v in enumerate(accuracy_rates):
#         plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')

#     plt.tight_layout()
#     plt.show()

# plot_accuracy_rates(results)